{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal House Price Valuation — Preprocessing & EDA\n",
        "\n",
        "This notebook sets up the **data foundations** for a multimodal house price model using:\n",
        "- **Tabular attributes** (King County housing data)\n",
        "- **Satellite imagery** (Sentinel Hub tiles around each property)\n",
        "\n",
        "The objectives here are:\n",
        "- To make our **economic and causal assumptions explicit**.\n",
        "- To perform **tabular + geospatial EDA** that challenges those assumptions.\n",
        "- To prepare **clean, leakage-aware splits** for downstream modeling.\n",
        "\n",
        "We treat this as a production-grade analytical system, not a Kaggle notebook; every modeling choice must earn its right to exist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Economic and Causal Assumptions\n",
        "\n",
        "We model **transaction prices** as an equilibrium outcome of supply and demand, conditional on both **property attributes** and **neighbourhood quality**.\n",
        "\n",
        "Tabular variables (bedrooms, bathrooms, square footage, grade, condition, waterfront, view, latitude/longitude, etc.) capture only part of this structure. They typically miss:\n",
        "\n",
        "- **Greenery and open space**: tree canopy, parks, and visual openness vs. concrete.\n",
        "- **Water proximity and views**: lakes, rivers, ocean, marinas beyond a coarse `waterfront` flag.\n",
        "- **Road density and accessibility**: cul-de-sacs vs. arterial roads, distance to main corridors.\n",
        "- **Urban texture and density**: detached houses vs. townhouses vs. apartment blocks.\n",
        "- **Neighbourhood planning and layout**: block shapes, lot regularity, street grid vs. organic patterns.\n",
        "\n",
        "Satellite imagery may act as a **proxy** for these latent neighbourhood factors. We are **not** claiming pixels cause higher prices; instead, we test whether visual context contains **predictive information** beyond the tabular features.\n",
        "\n",
        "Key risks and caveats:\n",
        "- **Spatial confounding**: high-demand areas may have both higher prices and better amenities; imagery cannot de-confound this.\n",
        "- **Measurement error** in coordinates and imagery (clouds, acquisition time, seasonal effects).\n",
        "- **Selection bias** if our imagery coverage differs systematically across locations.\n",
        "\n",
        "Our evaluation will therefore focus on **out-of-sample predictive performance** and **stability across spatial splits**, not causal effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Paths\n",
        "\n",
        "We assume the project root contains the following files provided by you:\n",
        "\n",
        "- `train.csv` – training data with the **target price**.\n",
        "- `test.xlsx` – test data **without price** (features only, for blind prediction).\n",
        "- `APIcredentials/` – text file with Sentinel Hub credentials (used via `.env` or environment variables).\n",
        "\n",
        "To make the notebook robust, we:\n",
        "- Look for data under `data/raw/` if you follow the recommended structure.\n",
        "- Fall back to the project root if the files are there instead.\n",
        "\n",
        "If your column names differ from the canonical King County dataset (e.g., `price`, `id`, `lat`, `long`), adjust the configuration cell below accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import geopandas as gpd\n",
        "import contextily as ctx\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "SATELLITE_DIR = PROJECT_ROOT / \"data\" / \"satellite\"\n",
        "EMBEDDINGS_DIR = PROJECT_ROOT / \"data\" / \"embeddings\"\n",
        "PROVIDED_DIR = PROJECT_ROOT / \"Provided\"\n",
        "\n",
        "for d in [RAW_DIR, PROCESSED_DIR, SATELLITE_DIR, EMBEDDINGS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_candidates = [\n",
        "    RAW_DIR / \"train.csv\",\n",
        "    PROJECT_ROOT / \"train.csv\",\n",
        "    PROVIDED_DIR / \"train.csv\",\n",
        "]\n",
        "\n",
        "test_candidates = [\n",
        "    RAW_DIR / \"test.xlsx\",\n",
        "    PROJECT_ROOT / \"test.xlsx\",\n",
        "    PROVIDED_DIR / \"test.xlsx\",\n",
        "]\n",
        "\n",
        "for p in train_candidates:\n",
        "    if p.exists():\n",
        "        TRAIN_PATH = p\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find train.csv in {train_candidates}. Please adjust paths.\"\n",
        "    )\n",
        "\n",
        "for p in test_candidates:\n",
        "    if p.exists():\n",
        "        TEST_PATH = p\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find test.xlsx in {test_candidates}. Please adjust paths.\"\n",
        "    )\n",
        "\n",
        "print(\"Using TRAIN_PATH =\", TRAIN_PATH)\n",
        "print(\"Using TEST_PATH =\", TEST_PATH)\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_excel(TEST_PATH)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure key column names (adjust here if your schema differs)\n",
        "\n",
        "# Attempt to infer common column names; override manually if needed.\n",
        "\n",
        "def infer_first_present(candidates, columns, default=None):\n",
        "    for c in candidates:\n",
        "        if c in columns:\n",
        "            return c\n",
        "    return default\n",
        "\n",
        "cols = train_df.columns\n",
        "\n",
        "TARGET_COL = infer_first_present([\"price\", \"SalePrice\", \"y\"], cols)\n",
        "ID_COL = infer_first_present([\"id\", \"Id\", \"house_id\"], cols)\n",
        "LAT_COL = infer_first_present([\"lat\", \"latitude\"], cols)\n",
        "LON_COL = infer_first_present([\"long\", \"lon\", \"longitude\"], cols)\n",
        "\n",
        "if TARGET_COL is None:\n",
        "    raise ValueError(\n",
        "        \"Could not infer target column (price). Set TARGET_COL manually in this cell.\"\n",
        "    )\n",
        "if ID_COL is None:\n",
        "    raise ValueError(\n",
        "        \"Could not infer ID column. Set ID_COL manually in this cell.\"\n",
        "    )\n",
        "if LAT_COL is None or LON_COL is None:\n",
        "    raise ValueError(\n",
        "        \"Could not infer latitude/longitude columns. Set LAT_COL and LON_COL manually.\"\n",
        "    )\n",
        "\n",
        "print(\"TARGET_COL =\", TARGET_COL)\n",
        "print(\"ID_COL =\", ID_COL)\n",
        "print(\"LAT_COL =\", LAT_COL)\n",
        "print(\"LON_COL =\", LON_COL)\n",
        "\n",
        "# Create a log-price for more Gaussian-like residuals\n",
        "train_df[\"log_price\"] = np.log1p(train_df[TARGET_COL])\n",
        "\n",
        "train_df[[TARGET_COL, \"log_price\"]].describe().T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1️⃣ Tabular EDA: price distributions and key drivers\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(train_df[TARGET_COL], bins=50, ax=axes[0])\n",
        "axes[0].set_title(\"Raw price distribution\")\n",
        "axes[0].set_xlabel(TARGET_COL)\n",
        "\n",
        "sns.histplot(train_df[\"log_price\"], bins=50, ax=axes[1])\n",
        "axes[1].set_title(\"Log(price + 1) distribution\")\n",
        "axes[1].set_xlabel(\"log_price\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Simple relationship plots for selected drivers (if present)\n",
        "key_drivers = [\n",
        "    \"sqft_living\",\n",
        "    \"sqft_lot\",\n",
        "    \"sqft_living15\",\n",
        "    \"sqft_lot15\",\n",
        "    \"bedrooms\",\n",
        "    \"bathrooms\",\n",
        "    \"grade\",\n",
        "    \"condition\",\n",
        "]\n",
        "\n",
        "available_drivers = [c for c in key_drivers if c in train_df.columns]\n",
        "\n",
        "print(\"Using key drivers:\", available_drivers)\n",
        "\n",
        "fig, axes = plt.subplots(len(available_drivers), 1, figsize=(10, 4 * len(available_drivers)))\n",
        "if len(available_drivers) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, col in zip(axes, available_drivers):\n",
        "    sns.scatterplot(\n",
        "        data=train_df.sample(min(5000, len(train_df)), random_state=42),\n",
        "        x=col,\n",
        "        y=\"log_price\",\n",
        "        alpha=0.3,\n",
        "        ax=ax,\n",
        "    )\n",
        "    ax.set_title(f\"log_price vs {col}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1️⃣ Tabular EDA: correlation and multicollinearity\n",
        "\n",
        "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop obvious non-features from correlation view\n",
        "for col in [ID_COL]:\n",
        "    if col in numeric_cols:\n",
        "        numeric_cols.remove(col)\n",
        "\n",
        "corr = train_df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, square=True, cbar_kws={\"shrink\": 0.5})\n",
        "plt.title(\"Correlation matrix (numeric features)\")\n",
        "plt.show()\n",
        "\n",
        "# Variance Inflation Factor (VIF) for a subset of core regressors\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_features = [\n",
        "    c\n",
        "    for c in [\n",
        "        \"sqft_living\",\n",
        "        \"sqft_lot\",\n",
        "        \"sqft_living15\",\n",
        "        \"sqft_lot15\",\n",
        "        \"bedrooms\",\n",
        "        \"bathrooms\",\n",
        "        \"grade\",\n",
        "        \"condition\",\n",
        "    ]\n",
        "    if c in train_df.columns\n",
        "]\n",
        "\n",
        "if len(vif_features) > 1:\n",
        "    X_vif = train_df[vif_features].dropna().astype(float)\n",
        "    vif_data = []\n",
        "    for i, col in enumerate(vif_features):\n",
        "        vif_value = variance_inflation_factor(X_vif.values, i)\n",
        "        vif_data.append({\"feature\": col, \"VIF\": vif_value})\n",
        "    vif_df = pd.DataFrame(vif_data).sort_values(\"VIF\", ascending=False)\n",
        "    display(vif_df)\n",
        "else:\n",
        "    print(\"Not enough features for VIF calculation; adjust vif_features if needed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1️⃣ Tabular EDA: monotonicity sanity checks\n",
        "\n",
        "# We expect average log_price to increase (weakly) with size and quality.\n",
        "\n",
        "features_to_check = [\n",
        "    f for f in [\"sqft_living\", \"sqft_lot\", \"grade\", \"bathrooms\", \"bedrooms\"] if f in train_df.columns\n",
        "]\n",
        "\n",
        "def plot_monotonicity(feature, bins=10):\n",
        "    tmp = train_df[[feature, \"log_price\"]].dropna().copy()\n",
        "    try:\n",
        "        # Use quantile bins to get roughly equal-sized groups\n",
        "        tmp[\"bin\"] = pd.qcut(tmp[feature], q=bins, duplicates=\"drop\")\n",
        "    except ValueError:\n",
        "        # Fallback to simple bins for discrete features like grade\n",
        "        tmp[\"bin\"] = pd.cut(tmp[feature], bins=min(bins, tmp[feature].nunique()))\n",
        "    gp = tmp.groupby(\"bin\")[\"log_price\"].mean()\n",
        "    gp.plot(kind=\"bar\", figsize=(8, 4))\n",
        "    plt.title(f\"Average log_price by binned {feature}\")\n",
        "    plt.ylabel(\"mean(log_price)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for f in features_to_check:\n",
        "    plot_monotonicity(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geospatial EDA: spatial structure and leakage risk\n",
        "\n",
        "We now examine the **spatial structure** of prices using latitude/longitude:\n",
        "\n",
        "- Are high prices spatially clustered (e.g., along waterfronts or affluent suburbs)?\n",
        "- Do residuals from a simple tabular model show **spatial patterns**, indicating missing spatial features?\n",
        "- How should we define **train/validation splits** to avoid training on near-identical neighbours of test points?\n",
        "\n",
        "We use GeoPandas and a web-mercator projection to overlay price patterns on a basemap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2️⃣ Geospatial EDA: price maps and clustering intuition\n",
        "\n",
        "# Build a GeoDataFrame\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    train_df.copy(),\n",
        "    geometry=gpd.points_from_xy(train_df[LON_COL], train_df[LAT_COL]),\n",
        "    crs=\"EPSG:4326\",\n",
        ")\n",
        "\n",
        "# Project to web-mercator for plotting with contextily\n",
        "\n",
        "gdf_web = gdf.to_crs(epsg=3857)\n",
        "\n",
        "# Subsample for faster plotting if dataset is large\n",
        "max_points = 8000\n",
        "if len(gdf_web) > max_points:\n",
        "    gdf_web_sample = gdf_web.sample(max_points, random_state=42)\n",
        "else:\n",
        "    gdf_web_sample = gdf_web\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "price_col = \"log_price\"\n",
        "\n",
        "gdf_web_sample.plot(\n",
        "    ax=ax,\n",
        "    column=price_col,\n",
        "    cmap=\"viridis\",\n",
        "    scheme=\"quantiles\",\n",
        "    k=10,\n",
        "    markersize=5,\n",
        "    alpha=0.7,\n",
        "    legend=True,\n",
        ")\n",
        "\n",
        "# Use a widely available tile provider; some Stamen tiles are deprecated in newer contextily versions\n",
        "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
        "ax.set_axis_off()\n",
        "ax.set_title(\"Spatial distribution of log_price (sampled)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Simple spatial clustering proxy: bin lat/long into a grid to approximate neighbourhoods\n",
        "\n",
        "grid_size_deg = 0.02  # ~2 km; adjust if your area is more compact or spread out\n",
        "\n",
        "lat_bin = (train_df[LAT_COL] / grid_size_deg).round().astype(int)\n",
        "lon_bin = (train_df[LON_COL] / grid_size_deg).round().astype(int)\n",
        "\n",
        "train_df[\"spatial_bin\"] = lat_bin.astype(str) + \"_\" + lon_bin.astype(str)\n",
        "\n",
        "print(\"Number of spatial bins:\", train_df[\"spatial_bin\"].nunique())\n",
        "train_df[\"spatial_bin\"].value_counts().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual EDA: satellite tiles by price quantile\n",
        "\n",
        "Before training any image model, we want to **look at the data like a human appraiser**:\n",
        "\n",
        "- For low-, mid-, and high-priced properties, what does the surrounding environment look like?\n",
        "- Do expensive homes visually cluster near **water**, **greenery**, or low-density cul-de-sacs?\n",
        "- Are there systematic artefacts (clouds, missing tiles) that could mislead a model?\n",
        "\n",
        "We sample satellite tiles (if already fetched via `data_fetcher.py`) and group them by price quantiles to form initial **visual hypotheses**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "meta_path = SATELLITE_DIR / \"image_metadata.csv\"\n",
        "\n",
        "if not meta_path.exists():\n",
        "    print(\n",
        "        f\"No satellite metadata found at {meta_path}. \"\n",
        "        \"Run data_fetcher.py first to download Sentinel tiles.\"\n",
        "    )\n",
        "else:\n",
        "    meta_df = pd.read_csv(meta_path)\n",
        "    # Keep successful or cached images only\n",
        "    meta_df = meta_df[meta_df[\"status\"].isin([\"ok\", \"cached\"])]\n",
        "\n",
        "    merged = train_df.merge(\n",
        "        meta_df[[\"id\", \"image_path\"]].rename(columns={\"id\": ID_COL}),\n",
        "        on=ID_COL,\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # Define price quantiles\n",
        "    merged[\"price_quantile\"] = pd.qcut(\n",
        "        merged[TARGET_COL], q=[0.0, 0.2, 0.8, 1.0], labels=[\"low\", \"mid\", \"high\"]\n",
        "    )\n",
        "\n",
        "    def show_samples_for_quantile(label, n=9):\n",
        "        subset = merged[(merged[\"price_quantile\"] == label) & merged[\"image_path\"].notna()]\n",
        "        if subset.empty:\n",
        "            print(f\"No images found for quantile '{label}'.\")\n",
        "            return\n",
        "        sample = subset.sample(min(n, len(subset)), random_state=42)\n",
        "        n_cols = 3\n",
        "        n_rows = int(np.ceil(len(sample) / n_cols))\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "        axes = axes.flatten()\n",
        "        for ax, (_, row) in zip(axes, sample.iterrows()):\n",
        "            try:\n",
        "                img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "                ax.imshow(img)\n",
        "                ax.set_title(f\"{label} price\\nID={row[ID_COL]}\")\n",
        "                ax.axis(\"off\")\n",
        "            except Exception as e:  # noqa: BLE001\n",
        "                ax.text(0.5, 0.5, f\"Error: {e}\", ha=\"center\", va=\"center\")\n",
        "                ax.axis(\"off\")\n",
        "        for ax in axes[len(sample) :]:\n",
        "            ax.axis(\"off\")\n",
        "        plt.suptitle(f\"Satellite samples for {label}-priced properties\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    for q in [\"low\", \"mid\", \"high\"]:\n",
        "        show_samples_for_quantile(q, n=9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leakage-aware train/validation splits\n",
        "\n",
        "A naïve random train/test split on individual rows can **leak spatial information**:\n",
        "\n",
        "- Nearby properties (same street or block) share unobserved neighbourhood factors.\n",
        "- If one home is in train and its neighbour is in test, the model may **memorise local price levels** rather than learn generalisable relationships.\n",
        "\n",
        "To mitigate this, we:\n",
        "\n",
        "- Use the `spatial_bin` grid defined above as a proxy for neighbourhood clusters.\n",
        "- Apply a **group-wise split** so that all properties in a given spatial bin fall entirely in train or validation.\n",
        "- Keep your original `test.xlsx` as a final blind set (features only); we do **not** leak its information into training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# Ensure spatial_bin exists\n",
        "if \"spatial_bin\" not in train_df.columns:\n",
        "    raise RuntimeError(\"spatial_bin not defined; run the geospatial EDA cell first.\")\n",
        "\n",
        "groups = train_df[\"spatial_bin\"].astype(str)\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(train_df, groups=groups))\n",
        "\n",
        "train_split = train_df.iloc[train_idx].copy()\n",
        "val_split = train_df.iloc[val_idx].copy()\n",
        "\n",
        "print(\"Train split shape:\", train_split.shape)\n",
        "print(\"Validation split shape:\", val_split.shape)\n",
        "\n",
        "# Save processed splits for modeling notebook\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_split.to_parquet(PROCESSED_DIR / \"train.parquet\", index=False)\n",
        "val_split.to_parquet(PROCESSED_DIR / \"val.parquet\", index=False)\n",
        "\n",
        "print(\"Saved train/val splits to\", PROCESSED_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
