{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal House Price Valuation — Explainability & Economic Insight\n",
        "\n",
        "This notebook focuses on **explaining** our multimodal models, not just scoring them.\n",
        "\n",
        "We will:\n",
        "- Use **Grad-CAM (or similar)** to highlight which regions of satellite tiles drive predictions.\n",
        "- Compare attention patterns for **high vs. low priced** properties and high-error cases.\n",
        "- Cross-check with **tabular feature attributions** (e.g., SHAP) to understand how visual and tabular signals interact.\n",
        "- Reject models whose attention maps look **random or economically meaningless**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Assumptions\n",
        "\n",
        "This notebook assumes you have already run:\n",
        "\n",
        "- `preprocessing.ipynb` to create leakage-aware `train.parquet` / `val.parquet`.\n",
        "- `data_fetcher.py` to download Sentinel tiles and `data/satellite/image_metadata.csv`.\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Train a **lightweight image-only regressor** (frozen ResNet backbone + small regression head) on a subset of the data.\n",
        "- Use **Grad-CAM** on the last convolutional block to highlight regions most influential for price predictions.\n",
        "- Compare attention patterns across:\n",
        "  - High vs low priced properties.\n",
        "  - High-error vs low-error cases.\n",
        "\n",
        "This image-only model is used **only for interpretability**, not as the production predictor (which may be multimodal). The goal is to see whether it focuses on economically meaningful regions (water, greenery, roads, density).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "SATELLITE_DIR = PROJECT_ROOT / \"data\" / \"satellite\"\n",
        "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
        "\n",
        "for d in [REPORTS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load processed training data\n",
        "train_path = PROCESSED_DIR / \"train.parquet\"\n",
        "val_path = PROCESSED_DIR / \"val.parquet\"\n",
        "\n",
        "if not train_path.exists():\n",
        "    raise FileNotFoundError(f\"train.parquet not found at {train_path}; run preprocessing first.\")\n",
        "\n",
        "train_df = pd.read_parquet(train_path)\n",
        "val_df = pd.read_parquet(val_path) if val_path.exists() else None\n",
        "\n",
        "TARGET_COL = \"log_price\" if \"log_price\" in train_df.columns else \"price\"\n",
        "ID_COL = \"id\" if \"id\" in train_df.columns else \"Id\" if \"Id\" in train_df.columns else None\n",
        "\n",
        "if ID_COL is None:\n",
        "    raise ValueError(\"ID column not found in processed train data.\")\n",
        "\n",
        "print(\"TARGET_COL =\", TARGET_COL)\n",
        "print(\"ID_COL =\", ID_COL)\n",
        "\n",
        "meta_path = SATELLITE_DIR / \"image_metadata.csv\"\n",
        "if not meta_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Satellite metadata not found at {meta_path}; run data_fetcher.py first.\"\n",
        "    )\n",
        "\n",
        "meta_df = pd.read_csv(meta_path)\n",
        "meta_df = meta_df[meta_df[\"status\"].isin([\"ok\", \"cached\"])]\n",
        "if ID_COL != \"id\" and \"id\" in meta_df.columns:\n",
        "    meta_df = meta_df.rename(columns={\"id\": ID_COL})\n",
        "\n",
        "meta_df = meta_df[meta_df[\"image_path\"].apply(lambda p: Path(p).exists())]\n",
        "\n",
        "print(\"Usable satellite images:\", len(meta_df))\n",
        "\n",
        "# Merge train data with imagery\n",
        "\n",
        "train_img_df = train_df.merge(meta_df[[ID_COL, \"image_path\"]], on=ID_COL, how=\"inner\")\n",
        "\n",
        "print(\"Train with images:\", train_img_df.shape)\n",
        "\n",
        "# To keep runtime manageable on CPU, we can subsample for Grad-CAM training\n",
        "max_train_images = 5000\n",
        "if len(train_img_df) > max_train_images:\n",
        "    train_img_df = train_img_df.sample(max_train_images, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Subsampled to {len(train_img_df)} images for Grad-CAM model training.\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "img_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "class ImagePriceDataset(Dataset):\n",
        "    def __init__(self, df, id_col, target_col, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.id_col = id_col\n",
        "        self.target_col = target_col\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):  # noqa: D401\n",
        "        \"\"\"Number of image-price pairs.\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        target = float(row[self.target_col])\n",
        "        sample_id = row[self.id_col]\n",
        "        return img, target, sample_id\n",
        "\n",
        "\n",
        "train_dataset = ImagePriceDataset(train_img_df, id_col=ID_COL, target_col=TARGET_COL, transform=img_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a lightweight image-only regressor (frozen backbone)\n",
        "\n",
        "try:\n",
        "    base_resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "except AttributeError:\n",
        "    base_resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in base_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# We will unfreeze only the final fully-connected head\n",
        "feature_dim = base_resnet.fc.in_features\n",
        "\n",
        "class ResNetRegressor(nn.Module):\n",
        "    def __init__(self, backbone, feature_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.head = nn.Linear(feature_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        out = self.head(feats)\n",
        "        return out.squeeze(-1), feats\n",
        "\n",
        "\n",
        "model = ResNetRegressor(base_resnet, feature_dim=feature_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.head.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def train_image_regressor(num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = []\n",
        "        for imgs, targets, _ in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds, _ = model(imgs)\n",
        "            loss = criterion(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - MSE: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "\n",
        "# Optionally train (or retrain) the image-only regressor before Grad-CAM\n",
        "# This can be commented out once a model is trained and saved.\n",
        "train_image_regressor(num_epochs=3)\n",
        "\n",
        "# Save model for reuse\n",
        "image_model_path = REPORTS_DIR / \"image_regressor_resnet18.pt\"\n",
        "torch.save(model.state_dict(), image_model_path)\n",
        "print(\"Saved image regressor to\", image_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grad-CAM implementation for regression\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, inputs, output):  # noqa: D401, ANN001\n",
        "            \"\"\"Store activations from the target layer.\"\"\"\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):  # noqa: D401, ANN001\n",
        "            \"\"\"Store gradients from the target layer.\"\"\"\n",
        "            self.gradients = grad_output[0].detach()\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.model.eval()\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        x = x.requires_grad_(True)\n",
        "        preds, _ = self.model(x)\n",
        "\n",
        "        # For regression, backpropagate from the scalar prediction\n",
        "        preds.mean().backward()\n",
        "\n",
        "        gradients = self.gradients  # (B, C, H, W)\n",
        "        activations = self.activations  # (B, C, H, W)\n",
        "\n",
        "        # Global average pooling over gradients\n",
        "        weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
        "\n",
        "        cam = (weights * activations).sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
        "        cam = torch.relu(cam)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        cam = cam.squeeze(0).squeeze(0).cpu().numpy()\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam\n",
        "\n",
        "\n",
        "def overlay_cam_on_image(img_np, cam, alpha=0.4):\n",
        "    \"\"\"Overlay a CAM heatmap onto an RGB image.\n",
        "\n",
        "    Args:\n",
        "        img_np: HxWx3 uint8 RGB image.\n",
        "        cam: HxW float32 mask in [0, 1].\n",
        "    \"\"\"\n",
        "    heatmap = (cam * 255).astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    overlay = (1 - alpha) * img_np.astype(float) + alpha * heatmap.astype(float)\n",
        "    overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "    return overlay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Grad-CAM object on the last convolutional block\n",
        "\n",
        "# For ResNet18, layer4 is the last conv block\n",
        "cam_extractor = GradCAM(model, target_layer=model.backbone.layer4[-1])\n",
        "\n",
        "\n",
        "def visualize_cam_for_row(row, title=None):\n",
        "    img_path = row[\"image_path\"]\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "    img_np = np.array(img_resized)\n",
        "\n",
        "    x = img_transform(img).unsqueeze(0).to(device)\n",
        "    cam = cam_extractor(x)\n",
        "\n",
        "    # Resize CAM to match image\n",
        "    cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n",
        "    overlay = overlay_cam_on_image(img_np, cam_resized)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(cam_resized, cmap=\"jet\")\n",
        "    axes[1].set_title(\"Grad-CAM mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title(\"Overlay\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    if title is not None:\n",
        "        fig.suptitle(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grad-CAM: High vs Low Prices and High-Error Cases\n",
        "\n",
        "We now inspect Grad-CAM maps for three types of properties:\n",
        "\n",
        "1. **High-price decile** – do maps focus on water, greenery, low-density areas, or premium amenities?\n",
        "2. **Low-price decile** – do maps highlight busy roads, high-density housing, or industrial areas?\n",
        "3. **High-error cases** – where the image-only model is most wrong; does it attend to misleading artefacts (clouds, borders, irrelevant land)?\n",
        "\n",
        "We visually compare these patterns to validate whether the model is using economically meaningful signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute predictions and residuals for train_img_df\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "all_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets, ids in DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0):\n",
        "        imgs = imgs.to(device)\n",
        "        targets = targets.to(device).float()\n",
        "        preds, _ = model(imgs)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_ids.extend(list(ids))\n",
        "\n",
        "preds = np.array(all_preds)\n",
        "targets = np.array(all_targets)\n",
        "\n",
        "residuals = targets - preds\n",
        "\n",
        "results_df = pd.DataFrame(\n",
        "    {\n",
        "        ID_COL: all_ids,\n",
        "        \"target\": targets,\n",
        "        \"pred\": preds,\n",
        "        \"residual\": residuals,\n",
        "    }\n",
        ")\n",
        "\n",
        "results_df = results_df.merge(train_img_df[[ID_COL, \"image_path\"]], on=ID_COL, how=\"left\")\n",
        "\n",
        "# High- and low-price deciles (based on target)\n",
        "\n",
        "high_price = results_df.sort_values(\"target\", ascending=False).head(12)\n",
        "low_price = results_df.sort_values(\"target\", ascending=True).head(12)\n",
        "\n",
        "# High-error cases (absolute residuals)\n",
        "\n",
        "high_error = results_df.assign(abs_res=lambda d: d[\"residual\"].abs()).sort_values(\n",
        "    \"abs_res\", ascending=False\n",
        ").head(12)\n",
        "\n",
        "print(\"High price examples:\", len(high_price))\n",
        "print(\"Low price examples:\", len(low_price))\n",
        "print(\"High error examples:\", len(high_error))\n",
        "\n",
        "\n",
        "def visualize_group(df, group_name):\n",
        "    print(f\"\\n=== {group_name} ===\")\n",
        "    for _, row in df.iterrows():\n",
        "        price = row[\"target\"]\n",
        "        pred = row[\"pred\"]\n",
        "        resid = row[\"residual\"]\n",
        "        title = f\"{group_name} | ID={row[ID_COL]} | target={price:.1f}, pred={pred:.1f}, resid={resid:.1f}\"\n",
        "        visualize_cam_for_row(row, title=title)\n",
        "\n",
        "\n",
        "# Uncomment to generate Grad-CAM visualizations (can be many figures)\n",
        "# visualize_group(high_price, \"High price\")\n",
        "# visualize_group(low_price, \"Low price\")\n",
        "# visualize_group(high_error, \"High error\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tabular Feature Importance and SHAP (Optional but Recommended)\n",
        "\n",
        "To understand how **tabular and image-derived features** interact in the final multimodal model:\n",
        "\n",
        "- Load the best-performing tabular or fusion model from `model_training.ipynb` (e.g., via `joblib.load`).\n",
        "- Use **SHAP** on the tabular feature space (and, if feasible, image embedding dimensions) to quantify feature contributions.\n",
        "- Compare the relative impact of classical drivers (`sqft_living`, `grade`, `waterfront`, etc.) vs. image features.\n",
        "\n",
        "This complementary view helps ensure that any **visual premiums** (e.g., water, greenery) align with both Grad-CAM attention and tabular attributions, rather than being artefacts of a single model.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
